{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ccdfd6",
   "metadata": {},
   "source": [
    "# Optimize Storage (Parallel Copy to SSD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f40c4af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Detected 17 CPU cores. Using 68 threads for copying.\n",
      "Scanning volume and creating folder structure...\n",
      "Found 13454 files. Starting Transfer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13454/13454 [01:13<00:00, 182.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA TRANSFER COMPLETE! Using Fast Local Storage (/tmp/dataset)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "SOURCE_DIR = \"/data\"          \n",
    "DEST_DIR = \"/tmp/dataset\"     \n",
    "\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "MAX_WORKERS = num_cores * 4   \n",
    "print(f\"Detected {num_cores} CPU cores. Using {MAX_WORKERS} threads for copying.\")\n",
    "\n",
    "\n",
    "def copy_file(src, dst):\n",
    "    try:\n",
    "        shutil.copy2(src, dst)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(\"Scanning volume and creating folder structure...\")\n",
    "files_to_copy = []\n",
    "\n",
    "if not os.path.exists(SOURCE_DIR):\n",
    "    print(\"Error: Volume not found at /data\")\n",
    "else:\n",
    "    for root, dirs, files in os.walk(SOURCE_DIR):\n",
    "        if \".ipynb_checkpoints\" in root: continue\n",
    "        rel_path = os.path.relpath(root, SOURCE_DIR)\n",
    "        dest_path = os.path.join(DEST_DIR, rel_path)\n",
    "        os.makedirs(dest_path, exist_ok=True)\n",
    "        \n",
    "        for file in files:\n",
    "            files_to_copy.append((os.path.join(root, file), os.path.join(dest_path, file)))\n",
    "\n",
    "    print(f\"Found {len(files_to_copy)} files. Starting Transfer...\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        list(tqdm(executor.map(lambda p: copy_file(*p), files_to_copy), total=len(files_to_copy)))\n",
    "\n",
    "    print(\"\\nDATA TRANSFER COMPLETE! Using Fast Local Storage (/tmp/dataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e95380",
   "metadata": {},
   "source": [
    "# Imports & GPU Verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ecb994",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-30 07:48:20.423114: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-30 07:48:20.510018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-30 07:48:24.450748: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/usr/local/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "Num GPUs Available:  1\n",
      "Training Data Path: /tmp/dataset/train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "DATA_ROOT = \"/tmp/dataset\"\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "VAL_DIR = os.path.join(DATA_ROOT, \"val\")\n",
    "\n",
    "if not os.path.exists(TRAIN_DIR):\n",
    "    TRAIN_DIR = DATA_ROOT\n",
    "    VAL_DIR = DATA_ROOT\n",
    "    print(f\"'train' folder not found. Using root: {TRAIN_DIR}\")\n",
    "else:\n",
    "    print(f\"Training Data Path: {TRAIN_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f686bcb",
   "metadata": {},
   "source": [
    "# Fast Data Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def631c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Class Ready.\n"
     ]
    }
   ],
   "source": [
    "class VideoGenerator(Sequence):\n",
    "    def __init__(self, data_dir, classes, batch_size, img_size, seq_len):\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = classes\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.seq_len = seq_len\n",
    "        self.class_map = {cls: i for i, cls in enumerate(classes)}\n",
    "        self.num_classes = len(classes)\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(self.data_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                for vid in os.listdir(cls_path):\n",
    "                    self.files.append(os.path.join(cls_path, vid))\n",
    "                    self.labels.append(self.class_map[cls])\n",
    "        \n",
    "        if len(self.files) > 0:\n",
    "            combined = list(zip(self.files, self.labels))\n",
    "            np.random.shuffle(combined)\n",
    "            self.files[:], self.labels[:] = zip(*combined)\n",
    "        else:\n",
    "            print(f\"Warning: No files found in {data_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.files) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x_paths = self.files[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        batch_y_labels = self.labels[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        \n",
    "        X = []\n",
    "        for vid_path in batch_x_paths:\n",
    "            frames = self.process_video(vid_path)\n",
    "            X.append(frames)\n",
    "        \n",
    "        return np.array(X), to_categorical(batch_y_labels, num_classes=self.num_classes)\n",
    "\n",
    "    def process_video(self, path):\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frames = []\n",
    "        try:\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            skip = max(int(total_frames / self.seq_len), 1)\n",
    "            \n",
    "            for i in range(self.seq_len):\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, i * skip)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: break\n",
    "                frame = cv2.resize(frame, (self.img_size, self.img_size))\n",
    "                frames.append(frame)\n",
    "        finally:\n",
    "            cap.release()\n",
    "            \n",
    "        while len(frames) < self.seq_len:\n",
    "            frames.append(np.zeros((self.img_size, self.img_size, 3)))\n",
    "            \n",
    "        return np.array(frames[:self.seq_len]) / 255.0\n",
    "\n",
    "print(\"Generator Class Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c732a",
   "metadata": {},
   "source": [
    "# Build CNN + LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbb178",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes Found (101): ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767080914.797223      21 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20817 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:ca:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ time_distributed                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ time_distributed_1              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">344,320</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,565</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ time_distributed                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m) â”‚     \u001b[38;5;34m2,257,984\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mTimeDistributed\u001b[0m)               â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ time_distributed_1              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mTimeDistributed\u001b[0m)               â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚       \u001b[38;5;34m344,320\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m101\u001b[0m)            â”‚         \u001b[38;5;34m6,565\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,608,869</span> (9.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,608,869\u001b[0m (9.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">350,885</span> (1.34 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m350,885\u001b[0m (1.34 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "IMG_SIZE = 128\n",
    "SEQ_LENGTH = 16\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "classes = sorted([d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))])\n",
    "print(f\"Classes Found ({len(classes)}): {classes[:5]}...\")\n",
    "\n",
    "train_gen = VideoGenerator(TRAIN_DIR, classes, BATCH_SIZE, IMG_SIZE, SEQ_LENGTH)\n",
    "val_gen = VideoGenerator(VAL_DIR, classes, BATCH_SIZE, IMG_SIZE, SEQ_LENGTH)\n",
    "\n",
    "def build_model():\n",
    "    cnn = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    cnn.trainable = False  \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(cnn, input_shape=(SEQ_LENGTH, IMG_SIZE, IMG_SIZE, 3)))\n",
    "    model.add(TimeDistributed(GlobalAveragePooling2D()))\n",
    "    \n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(len(classes), activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbdb895",
   "metadata": {},
   "source": [
    "# Train with Multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c63e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up High-Speed Data Pipeline...\n",
      "Building Parallel Pipelines...\n",
      "STARTING TRAINING \n",
      "Epoch 1/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915ms/step - accuracy: 0.1396 - loss: 3.8562\n",
      "Epoch 1: val_accuracy improved from None to 0.43335, saving model to /data/best_model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: finished saving model to /data/best_model_checkpoint.h5\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 1s/step - accuracy: 0.1850 - loss: 3.6189 - val_accuracy: 0.4334 - val_loss: 2.9195\n",
      "Epoch 2/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728ms/step - accuracy: 0.3065 - loss: 2.9731\n",
      "Epoch 2: val_accuracy improved from 0.43335 to 0.53377, saving model to /data/best_model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: finished saving model to /data/best_model_checkpoint.h5\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 867ms/step - accuracy: 0.3359 - loss: 2.8375 - val_accuracy: 0.5338 - val_loss: 2.3637\n",
      "Epoch 3/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794ms/step - accuracy: 0.4072 - loss: 2.4633\n",
      "Epoch 3: val_accuracy improved from 0.53377 to 0.59474, saving model to /data/best_model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: finished saving model to /data/best_model_checkpoint.h5\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 971ms/step - accuracy: 0.4295 - loss: 2.3575 - val_accuracy: 0.5947 - val_loss: 1.9773\n",
      "Epoch 4/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910ms/step - accuracy: 0.4787 - loss: 2.1587\n",
      "Epoch 4: val_accuracy improved from 0.59474 to 0.64136, saving model to /data/best_model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: finished saving model to /data/best_model_checkpoint.h5\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 1s/step - accuracy: 0.4959 - loss: 2.0770 - val_accuracy: 0.6414 - val_loss: 1.7406\n",
      "Epoch 5/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5334 - loss: 1.8843\n",
      "Epoch 5: val_accuracy improved from 0.64136 to 0.67723, saving model to /data/best_model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: finished saving model to /data/best_model_checkpoint.h5\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 1s/step - accuracy: 0.5492 - loss: 1.8126 - val_accuracy: 0.6772 - val_loss: 1.5182\n",
      "Epoch 6/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962ms/step - accuracy: 0.5661 - loss: 1.7118\n",
      "Epoch 6: val_accuracy improved from 0.67723 to 0.70771, saving model to /data/best_model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: finished saving model to /data/best_model_checkpoint.h5\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 1s/step - accuracy: 0.5873 - loss: 1.6387 - val_accuracy: 0.7077 - val_loss: 1.3314\n",
      "Epoch 7/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6175 - loss: 1.5314\n",
      "Epoch 7: val_accuracy improved from 0.70771 to 0.73939, saving model to /data/best_model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: finished saving model to /data/best_model_checkpoint.h5\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 1s/step - accuracy: 0.6276 - loss: 1.4747 - val_accuracy: 0.7394 - val_loss: 1.1974\n",
      "Epoch 8/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6427 - loss: 1.3706\n",
      "Epoch 8: val_accuracy improved from 0.73939 to 0.75672, saving model to /data/best_model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: finished saving model to /data/best_model_checkpoint.h5\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 1s/step - accuracy: 0.6543 - loss: 1.3310 - val_accuracy: 0.7567 - val_loss: 1.0833\n",
      "Epoch 9/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6829 - loss: 1.2402\n",
      "Epoch 9: val_accuracy did not improve from 0.75672\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 1s/step - accuracy: 0.6954 - loss: 1.1940 - val_accuracy: 0.7537 - val_loss: 1.0095\n",
      "Epoch 10/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6989 - loss: 1.1341\n",
      "Epoch 10: val_accuracy improved from 0.75672 to 0.78063, saving model to /data/best_model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: finished saving model to /data/best_model_checkpoint.h5\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 1s/step - accuracy: 0.7103 - loss: 1.1085 - val_accuracy: 0.7806 - val_loss: 0.9132\n",
      "Epoch 11/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7326 - loss: 1.0291\n",
      "Epoch 11: val_accuracy improved from 0.78063 to 0.79558, saving model to /data/best_model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: finished saving model to /data/best_model_checkpoint.h5\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 2s/step - accuracy: 0.7398 - loss: 1.0052 - val_accuracy: 0.7956 - val_loss: 0.8952\n",
      "Epoch 12/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7444 - loss: 0.9760\n",
      "Epoch 12: val_accuracy improved from 0.79558 to 0.80275, saving model to /data/best_model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: finished saving model to /data/best_model_checkpoint.h5\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 2s/step - accuracy: 0.7541 - loss: 0.9425 - val_accuracy: 0.8027 - val_loss: 0.8153\n",
      "Epoch 13/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7585 - loss: 0.8891\n",
      "Epoch 13: val_accuracy improved from 0.80275 to 0.80454, saving model to /data/best_model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: finished saving model to /data/best_model_checkpoint.h5\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 1s/step - accuracy: 0.7695 - loss: 0.8620 - val_accuracy: 0.8045 - val_loss: 0.7798\n",
      "Epoch 14/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7717 - loss: 0.8430\n",
      "Epoch 14: val_accuracy improved from 0.80454 to 0.81470, saving model to /data/best_model_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: finished saving model to /data/best_model_checkpoint.h5\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m454s\u001b[0m 1s/step - accuracy: 0.7847 - loss: 0.8022 - val_accuracy: 0.8147 - val_loss: 0.7426\n",
      "Epoch 15/15\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7871 - loss: 0.7982\n",
      "Epoch 15: val_accuracy did not improve from 0.81470\n",
      "\u001b[1m315/315\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 1s/step - accuracy: 0.7924 - loss: 0.7846 - val_accuracy: 0.8027 - val_loss: 0.7386\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "print(\"Setting up High-Speed Data Pipeline...\")\n",
    "\n",
    "def load_video_wrapper(path_tensor, label_tensor):\n",
    "    path = path_tensor.numpy().decode('utf-8')\n",
    "    label = label_tensor.numpy()\n",
    "    \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        skip = max(int(total_frames / SEQ_LENGTH), 1)\n",
    "        for i in range(SEQ_LENGTH):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i * skip)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "            frame = frame.astype(np.float32) / 255.0\n",
    "            frames.append(frame)\n",
    "    finally:\n",
    "        cap.release()\n",
    "        \n",
    "    while len(frames) < SEQ_LENGTH:\n",
    "        frames.append(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32))\n",
    "        \n",
    "    return np.array(frames[:SEQ_LENGTH]), label\n",
    "\n",
    "def process_path(path, label):\n",
    "    video, label = tf.py_function(\n",
    "        func=load_video_wrapper,\n",
    "        inp=[path, label],\n",
    "        Tout=[tf.float32, tf.float32]\n",
    "    )\n",
    "    video.set_shape((SEQ_LENGTH, IMG_SIZE, IMG_SIZE, 3))\n",
    "    label.set_shape((len(classes),))\n",
    "    return video, label\n",
    "\n",
    "def create_dataset(generator):\n",
    "    paths = generator.files\n",
    "    labels = to_categorical(generator.labels, num_classes=len(classes))\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    \n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE) \n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "print(\"Building Parallel Pipelines...\")\n",
    "train_ds = create_dataset(train_gen)\n",
    "val_ds = create_dataset(val_gen)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"/data/best_model_checkpoint.h5\", \n",
    "    monitor='val_accuracy', \n",
    "    save_best_only=True, \n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"STARTING TRAINING \")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653c463",
   "metadata": {},
   "source": [
    "# Save Final Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe13c75",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Model saved to: /data/final_model_cnn_lstm.h5\n",
      "You can now stop the Modal app and download the model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_path = \"/data/final_model_cnn_lstm.h5\"\n",
    "model.save(save_path)\n",
    "\n",
    "with open(\"/data/classes.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(classes))\n",
    "\n",
    "print(f\"Success! Model saved to: {save_path}\")\n",
    "print(\"You can now stop the Modal app and download the model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
